{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(111)\n",
    "np.random.seed(111)\n",
    "\n",
    "BATCH_SIZE =32\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n",
    "test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n",
    "print(type(train_file))\n",
    "train_data = pd.read_table(train_file)\n",
    "test_data = pd.read_table(test_file)\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', cached_dir='bert_ckpt', do_lower_case =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tokenizer.encode_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(text = sentence, add_special_tokens = True, max_length = MAX_LEN, pad_to_max_length = True, return_attention_mask = True)\n",
    "    print(encoded_dict)\n",
    "    print(encoded_dict.keys())\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    print(f\"input_id : {input_id}\")\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    print(f\"attention_mask : {attention_mask}\")\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    print(f\"token_type_id : {token_type_id}\")\n",
    "\n",
    "\n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = []\n",
    "attention_masks, token_type_ids = [], []\n",
    "train_data_labels = []\n",
    "\n",
    "for train_sentence, train_label in tqdm(zip(train_data['document'], train_data['label']), total = len(train_data)):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sentence, MAX_LEN)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        \n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "\n",
    "        print(len(input_ids))\n",
    "    except Exception as e:\n",
    "\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "train_movie_input_ids = np.array(input_ids, dtype=int)\n",
    "\n",
    "\n",
    "train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_movie_token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_token_type_ids)\n",
    "train_data_labels = np.array(train_data_labels, dtype=int)\n",
    "\n",
    "print(f\"Sentences : {len(train_movie_input_ids), len(train_data_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "input_id = train_movie_input_ids[idx]\n",
    "attentionb_mask = train_movie_attention_masks[idx]\n",
    "token_type_id = train_movie_token_type_ids[idx]\n",
    "\n",
    "\n",
    "print(input_id)\n",
    "print(attentionb_mask)\n",
    "print(token_type_id)\n",
    "\n",
    "print(tokenizer.decode(input_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir = dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, kernel_initializer = tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range,),name='classifier')\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training= False):\n",
    "        outputs = self.bert(inputs, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training = training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "cls_model = TFBertClassifier(model_name= 'bert-base-multilingual-cased', dir_path='bert_ckpt', num_class=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer = optimizer, loss =loss, metrics= [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tf2_bert_naver_movie\\weights.h5Directory already\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\NCC_AD~1\\AppData\\Local\\Temp\\__autograph_generated_filev2b4o76f.py\", line 13, in tf__call\n        logits = ag__.converted_call(ag__.ld(self).calssifier, (ag__.ld(pooled_output),), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'tf_bert_classifier' (type TFBertClassifier).\n    \n    in user code:\n    \n        File \"C:\\Users\\ncc_admin\\AppData\\Local\\Temp\\ipykernel_7968\\877456597.py\", line 13, in call  *\n            logits = self.calssifier(pooled_output)\n    \n        AttributeError: 'TFBertClassifier' object has no attribute 'calssifier'\n    \n    \n    Call arguments received by layer 'tf_bert_classifier' (type TFBertClassifier):\n      • inputs=('tf.Tensor(shape=(None, 39), dtype=int32)', 'tf.Tensor(shape=(None, 39), dtype=int32)', 'tf.Tensor(shape=(None, 39), dtype=int32)')\n      • attention_mask=None\n      • token_type_ids=None\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_path\u001b[39m}\u001b[39;00m\u001b[39mDirectory create\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m cp_callback \u001b[39m=\u001b[39m ModelCheckpoint(checkpoint_path, monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, save_best_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, save_weights_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m history \u001b[39m=\u001b[39m cls_model\u001b[39m.\u001b[39;49mfit(train_movie_inputs, train_data_labels, epochs \u001b[39m=\u001b[39;49m NUM_EPOCHS, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, validation_split\u001b[39m=\u001b[39;49mVALID_SPLIT, callbacks\u001b[39m=\u001b[39;49m[es_callback, cp_callback])\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(history\u001b[39m.\u001b[39mhistory)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\NCC_AD~1\\AppData\\Local\\Temp\\__autograph_generated_filer0qdvwc5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\NCC_AD~1\\AppData\\Local\\Temp\\__autograph_generated_filev2b4o76f.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, attention_mask, token_type_ids, training)\u001b[0m\n\u001b[0;32m     11\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(outputs)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     12\u001b[0m pooled_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdropout, (ag__\u001b[39m.\u001b[39mld(pooled_output),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m---> 13\u001b[0m logits \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcalssifier, (ag__\u001b[39m.\u001b[39mld(pooled_output),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\NCC_AD~1\\AppData\\Local\\Temp\\__autograph_generated_filev2b4o76f.py\", line 13, in tf__call\n        logits = ag__.converted_call(ag__.ld(self).calssifier, (ag__.ld(pooled_output),), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'tf_bert_classifier' (type TFBertClassifier).\n    \n    in user code:\n    \n        File \"C:\\Users\\ncc_admin\\AppData\\Local\\Temp\\ipykernel_7968\\877456597.py\", line 13, in call  *\n            logits = self.calssifier(pooled_output)\n    \n        AttributeError: 'TFBertClassifier' object has no attribute 'calssifier'\n    \n    \n    Call arguments received by layer 'tf_bert_classifier' (type TFBertClassifier):\n      • inputs=('tf.Tensor(shape=(None, 39), dtype=int32)', 'tf.Tensor(shape=(None, 39), dtype=int32)', 'tf.Tensor(shape=(None, 39), dtype=int32)')\n      • attention_mask=None\n      • token_type_ids=None\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_naver_movie\"\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "\n",
    "checkpoint_path = os.path.join(\"./\", model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"{checkpoint_path}Directory already\")\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "    print(f\"{checkpoint_path}Directory create\")\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor = 'val_accuracy', verbose = 1, save_best_only = True, save_weights_only = True)\n",
    "\n",
    "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs = NUM_EPOCHS, batch_size=BATCH_SIZE, validation_split=VALID_SPLIT, callbacks=[es_callback, cp_callback])\n",
    "\n",
    "print(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
